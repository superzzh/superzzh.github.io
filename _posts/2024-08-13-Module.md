---
title: Module
date: 2024-08-13 10:00:00 +0800
categories: [机器学习, Pytorch实用教程]
tags: [机器学习]
math: true
---

# 模型搭建简例


```python
import torch
import torch.nn as nn
```

这是一个简化的CNN模型，用于二分类。


```python
class TinnyCNN(nn.Module):
        def __init__(self, cls_num=2):
            super(TinnyCNN, self).__init__()
            self.convolution_layer = nn.Conv2d(1, 1, kernel_size=(3, 3))
            self.fc = nn.Linear(36, cls_num)

        def forward(self, x):
            x = self.convolution_layer(x)
            x = x.view(x.size(0), -1)
            out = self.fc(x)
            return out

model = TinnyCNN(2)
```

构建模型总共需要几步？三步！

- 写一个类继承自nn.Module
- 在init函数中，定义好需要用到的网络层
- 在forward函数中，编写模型执行逻辑：数据喂进来，先给谁处理，后给谁处理

# Module介绍

Module是所有神经网络的基类，所有的模型都必须继承于Module类，并且它可以嵌套，一个Module里可以包含另外一个Module。

Module定义了一些属性来管理模块的功能，分别用8个有序字典进行管理，分别是：

`self._modules = OrderedDict()`

`self._parameters = OrderedDict()`

`self._buffers = OrderedDict()`

`self._backward_hooks = OrderedDict()`

`self._forward_hooks = OrderedDict()`

`self._forward_pre_hooks = OrderedDict()`

`self._state_dict_hooks = OrderedDict()`

`self._load_state_dict_pre_hooks = OrderedDict()`

其中：

- `_modules`当中放着模型包含的网络层/模块
- `_parameters`当中放着模型参数

## forward函数

`forward()`之于Module，等价于`__getitem__`之于Dataset。forward函数是模型每次调用的具体实现，数据到底是怎么计算的在此定义。

所有的模型必须实现forward函数，否则调用时会报错。

通常会在这里调用其他module来完成数据的处理，例如使用`nn.Conv2d`来进行卷积操作，除了使用module对象，其它的数学运算、功能函数（如`torch.nn.functionals`里的系列函数）、for循环等都是可以使用的。 

注意：

- 一些激活函数没有可训练参数，也不是module类，因此会在forward函数中直接调用，而不需要在init中初始化。比如 ：`out = F.relu(self.conv1(x))`中的`F.relu`。
- forward函数中需要注意前后层数据的格式，上一层的输出一定要对得上下一层的输入，否则会报错，常见的报错是Linear层接收到了不合适的数据。

## 简单总结

- Module是所有模型的基类
- 每个module有8个字典管理它的核心属性
- 一个module可以包含多个子module
- 一个module相当于一个运算，必须实现forward函数

# 前向传播推理过程

模型的调用方法为：`outputs = model(data)`。可见实际上调用了类的魔术方法`___call__()`，说明`___call__()`封装了`forward()`函数。

# Parameter介绍

在Module中有一个重要的对象：Parameter，参数。它继承于Tensor，与Tensor差别不太大，主要作用是用来区分可训练的参数与常规的Tensor。

Module中对于参数是采用`_parameters`进行管理的，并且提供相应的api可以对module内所有参数进行调用与读取。

TinyCNN的优化器实现为：`optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)`，就是将模型的参数传递给优化器，让其进行更新。

# Module容器

## Sequential


```python
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Using Sequential with OrderedDict. This is functionally the
# same as the above code
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))
```

在执行中，Sequential被当做一个整体，一个独立的Module，依次调用当中的网络层。

## ModuleList


```python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])
        # self.linears = [nn.Linear(10, 10) for i in range(10)]    # 观察model._modules，将会是空的

    def forward(self, x):
        for sub_layer in self.linears:
            x = sub_layer(x)
        return x
```

## ModuleDict

ModuleDict就是可以像python的Dict一样为每个层赋予名字，可以根据网络层的名字进行选择性的调用网络层。


```python
class MyModule2(nn.Module):
    def __init__(self):
        super(MyModule2, self).__init__()
        self.choices = nn.ModuleDict({
                'conv': nn.Conv2d(3, 16, 5),
                'pool': nn.MaxPool2d(3)
        })
        self.activations = nn.ModuleDict({
                'lrelu': nn.LeakyReLU(),
                'prelu': nn.PReLU()
        })

    def forward(self, x, choice, act):
        x = self.choices[choice](x)
        x = self.activations[act](x)
        return x
```

## ParameterList & ParameterDict


```python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterDict({
                'left': nn.Parameter(torch.randn(5, 10)),
                'right': nn.Parameter(torch.randn(5, 10))
        })

    def forward(self, x, choice):
        x = self.params[choice].mm(x)
        return x

# ParameterList
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

    def forward(self, x):
        # ParameterList can act as an iterable, or be indexed using ints
        for i, p in enumerate(self.params):
            x = self.params[i // 2].mm(x) + p.mm(x)
        return x
```

# Module常用API

## 设置模型存放在cpu/gpu

基础使用


```python
import torch
import torch.nn as nn

net = nn.Sequential(nn.Linear(3, 3))

print("\nid:{} is_cuda: {}".format(id(net), next(net.parameters()).is_cuda))

net.cuda()
print("\nid:{} is_cuda: {}".format(id(net), next(net.parameters()).is_cuda))

net.cpu()
print("\nid:{} is_cuda: {}".format(id(net), next(net.parameters()).is_cuda))
```

    
    id:2235286002224 is_cuda: False
    
    id:2235286002224 is_cuda: True
    
    id:2235286002224 is_cuda: False


to 方法的妙用：根据当前平台是否支持cuda加速，自动选择


```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch
import torch.nn as nn

net = nn.Sequential(nn.Linear(3, 3))
print("\nid:{} is_cuda: {}".format(id(net), next(net.parameters()).is_cuda))

net.to(device)
print("\nid:{} is_cuda: {}".format(id(net), next(net.parameters()).is_cuda))
```

    
    id:1329468901248 is_cuda: False
    
    id:1329468901248 is_cuda: True


## 获取模型参数、加载权重参数

### state_dict


```python
state_dict = model.state_dict()
for key, parameter_value in state_dict.items():
    print(key)
    print(parameter_value, end="\n\n")
```

    convolution_layer.weight
    tensor([[[[-0.0586,  0.2958,  0.0794],
              [-0.0975, -0.1863, -0.2886],
              [ 0.0428, -0.0775, -0.0212]]]])
    
    convolution_layer.bias
    tensor([-0.0820])
    
    fc.weight
    tensor([[ 0.1165,  0.0235, -0.0438,  0.1110,  0.1489, -0.0167, -0.0951,  0.0331,
              0.1470, -0.0072,  0.0420, -0.0689,  0.0828,  0.0833,  0.1161,  0.0625,
              0.1383,  0.0415, -0.0853, -0.0155, -0.0974,  0.1354, -0.0970,  0.1201,
              0.0051,  0.1168, -0.0631, -0.0012, -0.1244,  0.0307,  0.1647,  0.1610,
             -0.0011, -0.0883,  0.0176, -0.1347],
            [ 0.1008, -0.0250,  0.0644,  0.0470,  0.1096, -0.0654,  0.0835,  0.1284,
             -0.0624,  0.0111, -0.0449, -0.1071,  0.1286, -0.1086, -0.0395,  0.1416,
              0.1321, -0.1113, -0.0998, -0.0708, -0.0049,  0.0485, -0.1228,  0.0476,
              0.1246,  0.1118, -0.1652, -0.0927, -0.1385,  0.1040, -0.1084, -0.0119,
             -0.0311, -0.0928, -0.0599,  0.1003]])
    
    fc.bias
    tensor([0.0320, 0.1332])
    


### load_state_dict


```python
state_dict_tinnycnn = model.state_dict()

state_dict_tinnycnn["convolution_layer.weight"][0, 0, 0, 0] = 12345. # 假设经过训练，权重参数发现变化

model.load_state_dict(state_dict_tinnycnn)  # 再次查看，发现权重第一个数改成了12345

for key, parameter_value in model.state_dict().items():
    print(key)
    print(parameter_value, end="\n\n")
```

    convolution_layer.weight
    tensor([[[[ 1.2345e+04,  6.9663e-02, -3.3315e-01],
              [ 4.5751e-02, -2.0935e-01, -7.2836e-02],
              [-2.0377e-01,  2.5844e-02,  1.8354e-01]]]])
    
    convolution_layer.bias
    tensor([-0.2796])
    
    fc.weight
    tensor([[ 0.0340, -0.0017,  0.1107,  0.0161,  0.1544, -0.0496, -0.1366,  0.0746,
              0.1133,  0.1651,  0.0821, -0.1090, -0.0747,  0.0451,  0.0768,  0.1139,
             -0.1245,  0.1624, -0.1453,  0.1289, -0.0792,  0.0212, -0.0440, -0.0205,
              0.1380, -0.0515,  0.1250,  0.0681, -0.0985, -0.0852,  0.1033, -0.1315,
             -0.0382, -0.0578, -0.0733, -0.1650],
            [ 0.0613,  0.1646,  0.0985, -0.0326, -0.1568,  0.1611, -0.0578, -0.1615,
             -0.0153, -0.0400, -0.0581,  0.1608, -0.0996, -0.1395, -0.0733, -0.0375,
             -0.1413,  0.1596, -0.0205, -0.1072,  0.1251, -0.1602,  0.1190, -0.1613,
              0.0160, -0.1234,  0.1439,  0.1425, -0.1128,  0.0535, -0.1587,  0.0040,
              0.0632, -0.1234,  0.0139, -0.0716]])
    
    fc.bias
    tensor([ 0.1440, -0.1561])
    


### load_state_dict常见报错


```python
from torchvision import models
alexnet = models.AlexNet()
alexnet.load_state_dict(state_dict_tinnycnn)
```


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-7-2b8705e621e0> in <module>
          1 from torchvision import models
          2 alexnet = models.AlexNet()
    ----> 3 alexnet.load_state_dict(state_dict_tinnycnn)
    

    D:\Anaconda_data\envs\pytorch_1.10_gpu\lib\site-packages\torch\nn\modules\module.py in load_state_dict(self, state_dict, strict)
       1481         if len(error_msgs) > 0:
       1482             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
    -> 1483                                self.__class__.__name__, "\n\t".join(error_msgs)))
       1484         return _IncompatibleKeys(missing_keys, unexpected_keys)
       1485 


    RuntimeError: Error(s) in loading state_dict for AlexNet:
    	Missing key(s) in state_dict: "features.0.weight", "features.0.bias", "features.3.weight", "features.3.bias", "features.6.weight", "features.6.bias", "features.8.weight", "features.8.bias", "features.10.weight", "features.10.bias", "classifier.1.weight", "classifier.1.bias", "classifier.4.weight", "classifier.4.bias", "classifier.6.weight", "classifier.6.bias". 
    	Unexpected key(s) in state_dict: "convolution_layer.weight", "convolution_layer.bias", "fc.weight", "fc.bias". 


可以看到对alexnet这个模型传入TinyCNN的state_dict，会得到两大报错：
* 第一种是alexnet需要的，但传进来的字典里没找到：分别是`features.0.weight`, `features.0.bias`等等
* 第二种是传进来的不是alexnet想要的，分别是`convolution_layer.weight`, `convolution_layer.bias`, `fc.weight`, `fc.bias`

# Module的模块、参数管理

### paramters、 named_parameters

- parameters：返回一个迭代器，迭代器可抛出Module的所有parameter对象
- named_parameters：作用同上，不仅可得到parameter对象，还会给出它的名称


```python
for param in model.parameters():
    print(type(param), param.size())
    print(param, end="\n\n")
```

    <class 'torch.nn.parameter.Parameter'> torch.Size([1, 1, 3, 3])
    Parameter containing:
    tensor([[[[-0.0293,  0.3131,  0.2536],
              [-0.0025,  0.2554, -0.1812],
              [ 0.0364, -0.2175, -0.2030]]]], requires_grad=True)
    
    <class 'torch.nn.parameter.Parameter'> torch.Size([1])
    Parameter containing:
    tensor([-0.0879], requires_grad=True)
    
    <class 'torch.nn.parameter.Parameter'> torch.Size([2, 36])
    Parameter containing:
    tensor([[-0.1614,  0.0160, -0.0019, -0.1576,  0.1080,  0.1188, -0.1602,  0.1221,
              0.1491, -0.1267,  0.1401, -0.0578,  0.0061, -0.1583, -0.0561, -0.0459,
             -0.1022, -0.0916,  0.1021,  0.0819, -0.1013, -0.0914, -0.0883, -0.0118,
              0.1662,  0.0689, -0.1063, -0.0247,  0.0218, -0.0604,  0.1546,  0.1394,
              0.1564,  0.0174,  0.0748,  0.0500],
            [ 0.1608,  0.1045,  0.1121,  0.1020,  0.0107, -0.1516, -0.1255, -0.0958,
             -0.1221,  0.0735, -0.0373, -0.1453,  0.0629, -0.1029,  0.1231, -0.1497,
             -0.1558,  0.1619, -0.1211,  0.0175,  0.0326, -0.1352, -0.0414, -0.0607,
              0.1268,  0.0350, -0.1254,  0.0396, -0.0382, -0.1332,  0.0506,  0.0931,
             -0.1273,  0.0354, -0.1320,  0.0272]], requires_grad=True)
    
    <class 'torch.nn.parameter.Parameter'> torch.Size([2])
    Parameter containing:
    tensor([ 0.0069, -0.0710], requires_grad=True)
    



```python
for name, param in model.named_parameters():
    print(name)
    print(param, end="\n\n")
```

    convolution_layer.weight
    Parameter containing:
    tensor([[[[-0.0293,  0.3131,  0.2536],
              [-0.0025,  0.2554, -0.1812],
              [ 0.0364, -0.2175, -0.2030]]]], requires_grad=True)
    
    convolution_layer.bias
    Parameter containing:
    tensor([-0.0879], requires_grad=True)
    
    fc.weight
    Parameter containing:
    tensor([[-0.1614,  0.0160, -0.0019, -0.1576,  0.1080,  0.1188, -0.1602,  0.1221,
              0.1491, -0.1267,  0.1401, -0.0578,  0.0061, -0.1583, -0.0561, -0.0459,
             -0.1022, -0.0916,  0.1021,  0.0819, -0.1013, -0.0914, -0.0883, -0.0118,
              0.1662,  0.0689, -0.1063, -0.0247,  0.0218, -0.0604,  0.1546,  0.1394,
              0.1564,  0.0174,  0.0748,  0.0500],
            [ 0.1608,  0.1045,  0.1121,  0.1020,  0.0107, -0.1516, -0.1255, -0.0958,
             -0.1221,  0.0735, -0.0373, -0.1453,  0.0629, -0.1029,  0.1231, -0.1497,
             -0.1558,  0.1619, -0.1211,  0.0175,  0.0326, -0.1352, -0.0414, -0.0607,
              0.1268,  0.0350, -0.1254,  0.0396, -0.0382, -0.1332,  0.0506,  0.0931,
             -0.1273,  0.0354, -0.1320,  0.0272]], requires_grad=True)
    
    fc.bias
    Parameter containing:
    tensor([ 0.0069, -0.0710], requires_grad=True)
    


### modules、named_modules

- modules：返回一个迭代器，迭代器可以抛出Module的所有Module对象，注意：模型本身也是module，所以也会获得自己。
- named_modules：作用同上，不仅可得到Module对象，还会给出它的名称


```python
for sub_module in model.modules():
    print(sub_module, end="\n\n")
```

    TinnyCNN(
      (convolution_layer): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
      (fc): Linear(in_features=36, out_features=2, bias=True)
    )
    
    Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
    
    Linear(in_features=36, out_features=2, bias=True)
    



```python
for name, sub_module in model.named_modules():
    print(name)
    print(sub_module, end="\n\n")
```

    
    TinnyCNN(
      (convolution_layer): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
      (fc): Linear(in_features=36, out_features=2, bias=True)
    )
    
    convolution_layer
    Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
    
    fc
    Linear(in_features=36, out_features=2, bias=True)
    


### children、named_children

- children：作用同modules，但不会返回Module自己。
- named_children：作用同named_modules，但不会返回Module自己。


```python
for sub_module in model.children():
    print(sub_module, end="\n\n")
```

    Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
    
    Linear(in_features=36, out_features=2, bias=True)
    



```python
for name, sub_module in model.named_children():
    print(name)
    print(sub_module, end="\n\n")
```

    convolution_layer
    Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
    
    fc
    Linear(in_features=36, out_features=2, bias=True)
    


### get_parameter、get_submodule


```python
print(model.get_parameter("fc.bias"))

print(model.get_submodule("convolution_layer"))

print(model.get_submodule("convolution_layer").get_parameter("bias")) # module还可以继续调用get_prameter
```

    Parameter containing:
    tensor([ 0.0069, -0.0710], requires_grad=True)
    Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
    Parameter containing:
    tensor([-0.0879], requires_grad=True)


## 设置模型的参数精度，可选半精度、单精度、双精度等


```python
model = TinnyCNN(2)
for name, param in model.named_parameters():
    print(param.dtype)
```

    torch.float32
    torch.float32
    torch.float32
    torch.float32



```python
model.half()
for name, param in model.named_parameters():
    print(param.dtype)
```


```python
model.float()
for name, param in model.named_parameters():
    print(param.dtype)
```


```python
model.double()
for name, param in model.named_parameters():
    print(param.dtype)
```


```python
model.bfloat16()
for name, param in model.named_parameters():
    print(param.dtype)
```

## 对子模块执行特定功能

apply


```python
@torch.no_grad()
def init_weights(m):

    if type(m) == nn.Linear:
        m.weight.fill_(1.0)
        
#         print(m.weight)
net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))

for param in net.parameters():
    print(param, end="\n\n")
    
net.apply(init_weights)

print("执行apply之后:")
for name, param in net.named_parameters():
    print(name)
    print(param, end="\n\n")
```

    Parameter containing:
    tensor([[-0.2534,  0.2375],
            [-0.4776, -0.2796]], requires_grad=True)
    
    Parameter containing:
    tensor([-0.5713,  0.1877], requires_grad=True)
    
    Parameter containing:
    tensor([[ 0.4486, -0.1789],
            [-0.4061, -0.0006]], requires_grad=True)
    
    Parameter containing:
    tensor([-0.6580,  0.6763], requires_grad=True)
    
    执行apply之后:
    0.weight
    Parameter containing:
    tensor([[1., 1.],
            [1., 1.]], requires_grad=True)
    
    0.bias
    Parameter containing:
    tensor([-0.5713,  0.1877], requires_grad=True)
    
    1.weight
    Parameter containing:
    tensor([[1., 1.],
            [1., 1.]], requires_grad=True)
    
    1.bias
    Parameter containing:
    tensor([-0.6580,  0.6763], requires_grad=True)
    


# 权重初始化

良好的模型权重初始化，有利于模型的训练，在`torch.nn.init`中提供了数十个初始化方法，如下例，对每一个子Module进行遍历，对不同的网络采取不同的初始化方法。


```python
for m in self.modules():
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.constant_(m.bias, 0)
```

## Xavier 系列

1. `torch.nn.init.xavieruniform(tensor, gain=1.0)`

2. `torch.nn.init.xaviernormal(tensor, gain=1.0)`

## Kaiming系列

1. `torch.nn.init.kaiminguniform(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`

2. `torch.nn.init.kaimingnormal(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`

## 常见系列

- 均匀分布初始化
    
  `torch.nn.init.uniform_(tensor, a=0, b=1)`

- 正态分布初始化

  `torch.nn.init.normal_(tensor, mean=0, std=1)`

- 常数初始化

  `torch.nn.init.constant_(tensor, val)`

- 单位矩阵初始化

  `torch.nn.init.eye_(tensor)`

- 正交初始化

  `torch.nn.init.orthogonal_(tensor, gain=1)`

- 稀疏初始化

  `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`

  sparsity：每一个column稀疏的比例，即为0的比例，设置每一列都有部分值为0

- 全零初始化

  `torch.nn.init.zeros_(tensor)`

- 全1初始化

  `torch.nn.init.ones_(tensor)`

- 狄拉克初始化

  `torch.nn.init.dirac_(tensor, groups=1)`

- 增益计算

  `torch.nn.init.calculate_gain(nonlinearity, param=None)`

  返回各激活函数对应的增益值，该值用于调整权重初始化的方差。
