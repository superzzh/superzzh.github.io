---
title: LLM Lesson2
date: 2024-07-01 12:50:00 +0800 
categories: [大语言模型工程师实训营, 暑期课程]
tags: [大模型]    
---

# 大模型的使用与微调方法——Qwen与LoRA

## 一、为什么要使用和微调开源大模型？

- 调用ChatGPT的问题（成本、稳定性）
- 预训练大模型的缺陷（幻觉、价值观）
- 领域适配（准确率）
    - 特定问题特定回答
- 企业级数据（隐私性）

## 二、如何部署开源大模型

- 接口API形式
- Web形式
- 开源大模型社区

## 三、如何微调开源大模型

### 3.1开源模型的加载

1. 全参数微调
2. LoRA
3. QLoRA

- modelscope:魔搭社区提供的开源工具，可用于开源模型下载
- transformers:大模型开源工具库，提供了模型加载、训练、生成等工具和方法

```python
from modelscope import snapshot download
from transformers import AutoModelForCausalLM, AutoTokenizer

# 从魔搭社区中下载开源模型
model_dir = snapshot_download('qwen/Qwen-7B', revision='v1.1.4', cache_dir='/data/suniian/Model/Qwen 7B')
# 加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
# 加载模型参数
model = AutoModelForCausalLM.from_pretrained(
                        model_dir,
                        device_map="auto',#自动将模型参数加载到GPU
                        offload_folder="offload",
                        trust_remote_code=True
                    ).eval()

# 配置模型生成参数(开源模型一般会附带generation_config文件)
model.generation_config = Generationconfig.from_pretrained(model_dir, trust_remote_code=True)
```

模型生成参数
: 控制模型生成内容特点，以及训练模式的参数，如`max_length`,`max_new_tokens`,`min_length`,`min_new_tokens`；`early_stopping`,`max_time`,`stop_strings`......

接着，便可以调用模型生成对话。在多轮对话中，使用`history`保存历史信息。

```python
response, history = model.chat(tokenizer, "浙江的省会在哪里?", history=history)
response, history = model.chat(tokenizer, "它有什么好玩的景点", history=history)
response, history = model.chat(tokenizer, "它有什么好玩的景点", history=history)
```
### 3.2模型微调

- transformers.BitsAndBytesConfig:模型参数类型配置

大模型量化概述<https://juejin.cn/post/7291931852800524329>

```python
from transformers import BitsAndBytesconfig
from transformers import AutoModelForCausalLM, AutoTokenizer

# 配置量化相关参数
quantization_config = BitsAndBytesconfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type='nf4"
            )
# 使用量化方式加载模型
model = model_cls.from_pretrained(
                model_path,
                device_map=device_map,#模型参数与GPU映射方式
                torch_dtype=model_dtype, #模型参数类型
                quantization_config=quantization_config, #量化配置参数
                trust_remote_code=True,
            )
```

- peft:用于Lora、QLora训练方法的工具库，简单易用

```python
# 使用QLora方式，对模型原始参数做量化处理
# QLora方式加载
model = prepare_model_for_kbit_training(model)
# 配置Lora的训练参数
lora_config = Loraconfig(
        r=8, # 控制Lora参数的维度k
        lora_alpha=16,
        lora_dropout=0.05.
        target_modules=target_modules, # 控制Lora参数的类型数量n
        task_type=lora_task_type
        )

# 生成Lora参数，加载model-lora模型
# 自动合并了model参数和LoRA参数
model = get_peft_model(medel,lora_cofig)
```

- datasets:训练数据预处理、批处理工具

```python
from datasets import load dataset
# 数据预处理，处理成特殊格式，
def generate_and tokenize prompt(data point):
    input_text="你现在是一个要索提取机器人，请根据要求从句子中提取出对应的关键元素。未发现对应的元素则不返回。\n<eoh>\n"
    input_text += '<|instruction|>:\n'+ data_point['instruction']+ '\n<eoh>\n'
    input_text +='<|input|>:\n'+ data_point['input']+ '\n<eoh>\n'
    input text += '<|output|>:\n'+ data_point['removed_output']+ '\n<eoa>\n'

    full_prompt = input_text[:max_len]
    tokenized_full_prompt = tokenize(full_prompt)
    return tokenized_full_prompt

data = load_dataset("json", data_files=data_path, split='train')
data = data.map(generate_and_tokenize_prompt, num_proc=num_proc)
```

- transformers.Trainer:深度学习模型训练器，自动完成模型训练过程

```python
from transformers import Trainer
# 加载Trainer训练器，自动配置优化器，数据集等
trainer = Trainer(
        model=model,
        train_dataset=train_data,
        eval_dataset=val_data,
        args=training_args,
        data_collator=DataCollatorForseq2seq(
                        tokenizer,
                        pad_to_multiple_of=8,
                        return_tensors="pt",
                        padding=True)
        )

# 开启训练过程
trainer.train(resume_from_checkpoint=False)
# 保存模型参数
model.save_pretrained(training_args.output_dir)
```

### 3.3微调模型的使用

- PeftModel:Lora模型加载工具，自动完成原模型参数、Lora参数的merge处理

```python
from peft import PeftModel
# 加载量化后的模型。（以什么方式训练，就要以什么方式加载）
base_model= AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            device_map=device_map,
            torch_dtype=model_dtype,
            quantization_config=quantization_config,
            trust_remote_code=True,
        )
# 加载model-lora。先加载模型原始参数，然后加载Lora参数。
model = PeftModel.from_pretrained(base_model, load_lora_parameter_dir, device_map='auto')
model.eval()
```

- model.generate、model.stream_generate:大模型生成调用工具，自动完成文本编码、模型调用生成、特殊token处理、编码解码等过程
- 整体输出与流式输出

```python
from transformers import Generationconfig
# 将输入文本进行编码
def tokenize_for_inference(prompt):
    inputs = tokenizer(prompt, return_tensors='pt')
    input_ids = inputs['input_ids'].cuda()
    return input_ids

# 按照训练方式，处理输入文本数据
def generate_and_tokenize_prompt_for_inference(data point):
    input_text = "你现在是一个要素振取机器人，请根据要求从句子中报取出对应的关键元素。未发现对应的元素则不返回。"
    input_text += "<|output|>:\n"
    tokenized_prompt = tokenize_for_inference(input_text)
    return tokenized_prompt

# 配置生成参数
generate_args = GenerationConfig(do_sample=True)
one_data = "这是一个要案提取的示例"
input_ids = generate_and_tokenize_prompt_for_inference(one_data)
# 模型生成并进行解码操作
with torch.no_grad():
    s = model.generate(input_ids=input_ids, generation_config=generate_args)
    output = tokenizer.decode(s[0], skip_special_tokens=True)
```

- gradio:大模型对话工具，自动搭建前后端，可用于构建简易的大模型交互界面，也可自动生成模型调用接口。

```python
import gradio as gr
# 配置gradio demo示例
demo = gr.ChatInterface(
    predict,
    title=tile,
    description=description
    ).queue()
# 运行demo示例
demo.launch(
    server_port=server_port,
    server_name=server_name
    )
```

## 四、其他问题

- 如何提升模型微调的效果？
    - 选用参数量更大的模型，或者新的模型(qwen-7B、qwen-14B、qwen-72B、qwen1.5)
    - 选用效果更好的微调方法(Lora、Qlora、全参数微调)
    - 提升数据质量、多样性(数据收集、清洗)
    - 调整微调参数
    ```python
    learning_rate # 学习率
    gradient_accumulation_steps # 参数更新频次
    num_train_epochs # 训练轮数
    lora_r # Lora参数-维度
    lora_alpha # Lora参数
    lora_dropout # Lora参数-过拟合
    ```
    - 调整训练策略（多任务混合训练、多轮问答）<https://arxiv.org/abs/2402.14658>
- 节省显存的策略
    - 直接使用量化版本的开源模型
    - 筛除冗余数据，减少tokens数量(即max_len)
    - 检査点 checkpoint 策略
- 如何提高训练速度
    - 使用多GPU，并行训练。
    - 调整参数 gradient_accumulation_steps
    - 使用 FlashAttention(开源微调框架 llama-factory 等)
- 如何评估模型的效果
    - 生成式任务:BLEU score(文本翻译)，Perplexity(问答任务)，modelEval
    - 分类任务:准确率(关系抽取)
    - 专项任务评估:准确率，比如代码能力评估(humaneval数据集)
- 微调是否有足够的数据？
- 微调大模型还是调整自己的提示模板？
    - 若能调整提示模板以提升效果，是最经济的方式
- 其他策略：RAG
