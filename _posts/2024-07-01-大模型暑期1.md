---
title: LLM Lesson1
date: 2024-07-01 12:50:00 +0800 
categories: [大语言模型应用, 暑期课程] 
tags: [大模型]    
---

# State of GPT and how to build it

## How to train a GPT assistant

Pretraining - SFT - Reward Modeling - RL

1. Predict the next word

    - Holmes said:"I see, the murderer is____" GPT needs to understrand the whole novel to fill the blank.
    - Code Ability -> Complex Inference Task
        - The charac of code: long, precise

2. Pretraining

    - 将所有的token作为预测目标，加和为损失函数
    - 基本上参数越多，Loss越小

3. SFT

    - Base Model are NOT assistant
    - 3h-principle->helpful, harmless, hornest
    - Less GPU needed
    - 使用一些问答对，让模型学会以正确方式学习

4. Reward Modeling

    - Alignment->让机器产生的结果与人类知识对齐
    - 给出三个答案，哪个答案更好？-> 给回答打分

5. Reinforcement Learning

    - 让模型总是输出分数最高的回答
    - 经过训练之后的模型，结果分布尖

## 大语言模型的科学性

神经元-----复杂系统/混沌-----> 大脑

Scaling Law，指导OpenAI训练GPT

> Loss下降一点点，实际效果都会提升非常多。
{: .prompt-tip }

- 能力涌现
    - 小模型(<10B)效果几乎随机
    - 大模型(>100B)效果显现

- 涌现
    - 模型表现是尖锐的，能力从无到有
    - 模型表现是无法预测的

- 能力涌现与Scaling Law是矛盾的吗？
    - 观察到的能力的尖锐、不可预测结果并不是某种能力的涌现，而是指标的不连续性、非线性导致的
    - 传统指标->平滑指标（Are Emergent Ability of Large Language Models a Mirage?）



