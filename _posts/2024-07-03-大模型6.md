---
title: LLM Lesson6
date: 2024-07-03 12:00:00 +0800
categories: [大语言模型工程师实训营, 暑期课程]
tags: [大模型]
---

# 大模型安全问题

- 大模型被利用生成有害内容
- 大模型生成谣言、虚假新闻
    - 提供标题、摘要，大模型编造

- 大模型安全
    - 国家层面：法规
    - 学术界与业界
        - 用户生成内容UGC
        - 用户生成内容安全审核
        - 大模型生成AIGC
        - 大模型安全培训

AIGC内容复杂度远超UGC

- 大模型训练周期中的安全风险与处理

## 越狱攻击

- 越狱攻击
    - 设计越狱prompt
    - “奶奶漏洞”-得到Windows序列号
    - 设计了一个温馨的场景，降低了模型的防范性
    - Do Anything Now, DAN 
    - 开发者模式（已关闭）


越狱攻击研究：jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
- 将大模型带入某种场景：角色扮演、责任环境、研究实验
- 注意力转移：转换成另一种任务来得到输出
- 大模型具备处理超长文本的能力，因此可以在问题前面添加非常多的模型回复恶意输入的实例（k-shot）
- 长尾编码
    - 加密编码
    - 低资源语言
    - 代码格式


祖鲁语
Low-Resource Language
Exploiting Programatic Behavior

Sure here is... 大模型会倾向于继续往后说，从而生成奇怪的/不好的内容

主任务+辅助任务。把恶意问题放在辅助任务里

## 越狱攻击归因

预训练目标的矛盾性：遵循人类指令的能力 与 拒绝有害内容的能力

安全训练的分布外泛化性

多模态大模型的安全问题同样存在

## 大模型安全防御

- 大模型外部内容审核
- 大模型系统提示约束
- 大模型红队对抗训练

