---
title: LLM Lesson5
date: 2024-07-02 17:00:00 +0800
categories: [大语言模型应用, 暑期课程]
tags: [大模型]
math: true
image:
  path: /images/大模型暑期课/LLM-Lesson5/cover.jpg
---

# 大模型对齐方法原理与在蚂蚁大模型的实践

## 对齐方法

1. 对齐的定义
    - 让LLM学习人类的价值观
    - 指令微调 和 Critique
    - 目标：有用、安全、可靠
        - 减少仇恨、种族歧视、违反人类价值观
        - 避免泄露关键数据、避免输出关于公司的负面评论
2. 为什么需要对齐？ 
    - 降低与原始预训练模型交互的成本。
    - 原始预训练模型只能通过few-shot等方式进行交互，而经过指令微调后，可以通过自然语言的方式与模型交互。
    - 但是，仅依靠指令微调，无法显式地对模型地错误进行纠正。原因：只告诉模型什么是对的，没有纠正回答中的错误。

    ![Desktop View](/images/LLM-Lesson5/DAN.jpg){: width="972" height="589" }
_通过扮演DAN来绕过ChatGPT的各种限制_


### RLHF（Reinforcement Learning from Human Feedback）：由人类对模型输出的结果进行排序，以习得人类偏好

1. SFT
    - 首先，再人类编写的数据上进行有监督学习
2. 奖励模型
    - 根据人类提供的排序结果训练奖励模型，用来判定模型输出答案的质量
    - 奖励分数越高，越推荐该答案
3. RL
    - 使用强化学习方法提升模型在奖励模型的得分
    ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p1.jpg){: width="972" height="589" }

- 强化学习
    - Agent与环境交互
    ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p2.jpg){: width="972" height="589" }
    - Agent根据Policy协议，按照目前的状态，做出最佳行为（使得奖励模型分数最高）
    - Policy：表示从状态到行动的映射函数，这里其实就是对齐后的大语言模型
    - 强化学习的目的：学习到最优的Policy，Agent在每一步采取行动时都能使得奖励最大化

- PPO（Proximal Policy Optimization）
    - 采用PPO算法对Policy进行优化，目标函数如下：
    $$ objective(\phi) = E_{(x,y)} \sim D_{\pi_{\phi}^{RL}} [r_{\theta}(x,y) - \beta log(\pi_{\phi}^{RL}(y|x) / \pi^{SFT}(y|x))] + \gamma E_{x \sim D_{pretrain}[log(\pi_{\phi}^{RL}(x))]}$$
    - 式中+号前的一部分是PPO原始的优化目标，其中完整的奖励函数$R(x,y)$为：
    $$ R(x,y) = r_{\theta}(x,y) - \beta log(\pi_{\phi}^{RL}(y|x) / \pi^{SFT}(y|x))$$
    - ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p3.jpg){: width="972" height="589" }

- 优点
    - 可以通过iterative的凡是对奖励模型和policy进行更新，保证模型效果的持续提升
- 缺点
    1. RLHF算法实现较为复杂，强化学习算法本身就存在调试困难、收敛稳定性差等问题，还需要将人类反馈和RL训练Pipeline进行融合
    2. 需要训练奖励模型，且对奖励模型的准确性有较高要求，所以奖励模型的大小可能需要超过policy模型的大小
    3. RLHF阶段所需的GPU显存较多，如果奖励模型和policy模型大小一样，那么所需的显存最多可能到SFT阶段的2-3倍

### DPO（Direct Preference Optimization）

- 必要性：RLHF存在实现复杂、资源占用高等问题，阻碍了该方法在实际应用中的推广和落地
- DPO提出了一个简洁的binary交叉熵目标函数，与RL的目标函数在数学上等价，但算法实现和训练更加简单，极大简化了奖励模型和RLHF的pipeline
- 目标函数如下，推导过程参考<https://zhuanlan.zhihu.com/p/676371444>
    $$L_{DPO} = -E_{(x,y_w,y_l) \sim D} [log \sigma (\beta log \frac{\pi_{\theta}(y_w|x)}{\pi_{ref}(y_w|x)} - \beta log \frac{\pi_{\theta}(y_l|x)}{\pi_{ref}(y_l|x)})]$$
    - $\sigma$指Sigmoid函数
- 效果
    - ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p4.jpg){: width="972" height="589" }

## 对齐实战

目标：提升大模型在金融领域的严谨性
![Desktop View](/images/大模型暑期课/LLM-Lesson5/p5.jpg){: width="972" height="589" }

- 训练数据：依照上述4个维度构造包含政府例的训练数据，超过30K
    - 示例：
    ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p6.jpg){: width="972" height="589" }
    - chosen-rejected正负例数据。上面的例子中强调，当前的数字仅仅是预测值。
    - 正负例数据如何生成？
        - 基于规则，将数字替换成错误的
        - 基于模型，用模型生成负例
        - 人工标注

- 产生问题
    1. 生成质量下降，容易出现重复

        ```text
        问题：兴银汇泓一年定期开放债券成立于哪一年？其资金规模大约是多少？

        模型答案：该基金成立于2022年02月，资金规模超过了两千亿两千零二00000000000000000000000000
        ```
    
    2. 拒绝回答比例增多，模型变得更加保守了



原因分析：DPO的优化目标是增大正例奖励$r_w$和负例奖励$r_l$的差值

![Desktop View](/images/大模型暑期课/LLM-Lesson5/p7.jpg){: width="972" height="589" }

正例奖励$r_w$理应为正，这样，当前policy下，正例概率大于负例；负例奖励$r_l$理应为负。但是式中并没有显式约束，使得这一条件成立。

而训练数据中正负例差别较小，所以会造成虽然正负例的差值在扩大，但是负例和正例的奖励同时下降。

- 解法一
    - ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p8.jpg){: width="972" height="589" }
    - ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p9.jpg){: width="972" height="589" }
- 解法二
    - ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p10.jpg){: width="972" height="589" }
- 最终结果
    ![Desktop View](/images/大模型暑期课/LLM-Lesson5/p11.jpg){: width="972" height="589" }





